{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8536624f",
   "metadata": {},
   "source": [
    "# Getting the brazilians funds holdings from CVM web site\n",
    "\n",
    "**What ?** The Brazilian funds are regulated by a governmental institution called CVM (Comissão de Valores Mobiliários). According to local law, every regulated fund needs to disclose its holdings ticker by ticker portfolio with a 90-day delay. This regulation promotes transparency in the Brazilian financial market and provides valuable information for analyzing fund strategies, thereby facilitating informed decisions regarding money allocation.\n",
    "\n",
    "**Why ?** \n",
    "\n",
    "**How ?** \n",
    "\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1UerOWXdGizjakNJVlGww2hKPmRUr4Eba\" alt=\"icon_mutual_funds_holdings\" width=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c243b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5a5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import csv\n",
    "import re\n",
    "import io\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e434b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "today = date.today().strftime(\"%Y%m%d\") # current data extraction\n",
    "today = '20240410' #******* manual paramenter test phase\n",
    "\n",
    "\n",
    "# path folders\n",
    "path_database = r\"C:/Users/lucas/OneDrive/FundsExplore/Databases/\"\n",
    "path_cda_hist_files = r\"C:/Users/lucas/OneDrive/FundsExplore/cda_hist_files_\"+today+\"/\"\n",
    "\n",
    "\n",
    "#%%\n",
    "#% ##############################################################################################################\n",
    "#%                                      1 -  CAPTURA COMPOSIÇÃO E DIVERSIFICAÇÃO DE CARTEIRA (CDA)\n",
    "#% ##############################################################################################################\n",
    "\n",
    "ano = '2024'\n",
    "mes = '03'\n",
    "url_raiz = f'https://dados.cvm.gov.br/dados/FI/DOC/CDA/DADOS/'\n",
    "url_file = 'cda_fi_'\n",
    "\n",
    "\n",
    "lista_datas = ['202403',\n",
    "                   '202402',\n",
    "                   '202401',\n",
    "                   '202312',\n",
    "                   '202311',\n",
    "                   '202310',\n",
    "                   '202309',\n",
    "                   '202308',\n",
    "                   '202307',\n",
    "                   '202306',\n",
    "                   '202305',\n",
    "                   '202304',\n",
    "                   '202303',\n",
    "                   '202302',\n",
    "                   '202301' ]\n",
    "\n",
    "#%% requests files from CVM web site and saving the \".zip\" files\n",
    "\n",
    "for data in lista_datas:\n",
    "    \n",
    "    # proxies = {\"https\": \"http://10.0.0.24:8080\"}\n",
    "    download = requests.get(url_raiz + url_file + data +'.zip') # proxies = proxies)\n",
    "    \n",
    "    # save the zip files at a folder\n",
    "    with open(path_cda_hist_files + url_file + data +'.zip',\"wb\") as arquivo_cvm:\n",
    "        \n",
    "        arquivo_cvm.write(download.content)\n",
    "       # arquivo_zip = zipfile.ZipFile(url_file + data +'.zip')\n",
    "\n",
    "\n",
    "#%% appending files on a final dataframe\n",
    "\n",
    "df_blc1_appd = pd.DataFrame()\n",
    "df_blc2_appd = pd.DataFrame()\n",
    "df_blc3_appd = pd.DataFrame()\n",
    "df_blc4_appd = pd.DataFrame()\n",
    "df_blc5_appd = pd.DataFrame()\n",
    "df_blc6_appd = pd.DataFrame()\n",
    "df_blc7_appd = pd.DataFrame()\n",
    "df_blc8_appd = pd.DataFrame()\n",
    "df_conf_appd = pd.DataFrame()\n",
    "df_fi_PL_appd = pd.DataFrame()\n",
    "df_fiim_confid_appd =  pd.DataFrame()\n",
    "df_fiim_appd =  pd.DataFrame()\n",
    "\n",
    "#listing all zip files path inside the reference folder\n",
    "all_files = os.listdir(path_cda_hist_files)\n",
    "zip_files_with_paths = [os.path.join(path_cda_hist_files, file) for file in all_files if file.endswith('.zip')]\n",
    "\n",
    "# Extract filenames from each ZipInfo .csv data object\n",
    "zip_info_list = arquivo_zip.filelist\n",
    "file_zip_names = [zip_info.filename for zip_info in zip_info_list]\n",
    "\n",
    "#%%\n",
    "\n",
    "def read_not_mandatory_csv_files(filename,arquivo_zip):\n",
    "    \n",
    "    df_filename = pd.DataFrame()\n",
    "   \n",
    "    #print(arquivo_zip.filelist)\n",
    "    try:\n",
    "        df_filename = pd.read_csv(arquivo_zip.open(filename),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "    except Exception as e:  \n",
    "        return print(\"**** \"+filename+\" not found ****\",e) \n",
    "    else:\n",
    "        df_filename = pd.read_csv(arquivo_zip.open(filename),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "\n",
    "    return df_filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "for cda_files in zip_files_with_paths:\n",
    "    \n",
    "    with zipfile.ZipFile(cda_files, 'r') as arquivo_zip:\n",
    "        data = re.search(r'(\\d{6})\\.zip',arquivo_zip.filename).group(1)\n",
    "  \n",
    "        #mandatory data\n",
    "        df_blc1 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_1_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc2 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_2_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc3 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_3_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc4 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_4_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc5 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_5_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc6 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_6_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc7 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_7_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_blc8 = pd.read_csv(arquivo_zip.open('cda_fi_BLC_8_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "        df_fi_PL= pd.read_csv(arquivo_zip.open('cda_fi_PL_'+data+'.csv'),sep = \";\", encoding = \"ISO-8859-1\",low_memory=False)\n",
    "      \n",
    "        #not mandatory or recent added data\n",
    "        df_conf = read_not_mandatory_csv_files('cda_fi_CONFID_'+data+'.csv',arquivo_zip)\n",
    "        df_fiim_confid = read_not_mandatory_csv_files('cda_fiim_CONFID_'+data+'.csv',arquivo_zip)\n",
    "        df_fiim = read_not_mandatory_csv_files('cda_fiim_'+data+'.csv',arquivo_zip)\n",
    "        \n",
    "        \n",
    "        df_blc1_appd = pd.concat([df_blc1, df_blc1_appd], ignore_index=True)\n",
    "        df_blc2_appd = pd.concat([df_blc2, df_blc2_appd], ignore_index=True)\n",
    "        df_blc3_appd = pd.concat([df_blc3, df_blc3_appd], ignore_index=True)\n",
    "        df_blc4_appd = pd.concat([df_blc4, df_blc4_appd], ignore_index=True)\n",
    "        df_blc5_appd = pd.concat([df_blc5, df_blc5_appd], ignore_index=True)\n",
    "        df_blc6_appd = pd.concat([df_blc6, df_blc6_appd], ignore_index=True)\n",
    "        df_blc7_appd = pd.concat([df_blc7, df_blc7_appd], ignore_index=True)\n",
    "        df_blc8_appd = pd.concat([df_blc8, df_blc8_appd], ignore_index=True)\n",
    "        df_fi_PL_appd = pd.concat([df_fi_PL,df_fi_PL_appd], ignore_index=True)\n",
    "        \n",
    "        df_conf_appd        = pd.concat([df_conf,df_conf_appd], ignore_index=True)\n",
    "        df_fiim_confid_appd = pd.concat([df_fiim_confid,df_fiim_confid_appd], ignore_index=True)\n",
    "        df_fiim_appd        = pd.concat([df_fiim,df_fiim_appd], ignore_index=True)\n",
    "  \n",
    "#%% ############################################################################################################\n",
    "#%                                      2 -  CAPTURA CADASTRO ATUALIZADO DOS FUNDOS\n",
    "#% ##############################################################################################################\n",
    "\n",
    "url_raiz_cad_fi = 'https://dados.cvm.gov.br/dados/FI/CAD/DADOS/cad_fi.csv'\n",
    "\n",
    "s = requests.get(url_raiz_cad_fi).text\n",
    "\n",
    "df_cad_fi = pd.read_csv(io.StringIO(s), sep = \";\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "\n",
    "#%%\n",
    "#% ##############################################################################################################\n",
    "#%                                      3 -  CRIANDO DATABASE SQL LITE\n",
    "#% ##############################################################################################################\n",
    "\n",
    "#criando database SQLite\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_connection(path_database + \"CVM_Database_\"+today+\".db\")\n",
    "\n",
    "#%%\n",
    "#salvando dados da CVM em tabelas neste dataframe\n",
    "conn = sqlite3.connect(path_database + \"CVM_Database_\"+today+\".db\") # abrindo conexão com database\n",
    "\n",
    "df_blc1_appd.to_sql('CDA_bloco_1',conn,if_exists='replace',index=False)\n",
    "df_blc2_appd.to_sql('CDA_bloco_2',conn,if_exists='replace',index=False)\n",
    "df_blc3_appd.to_sql('CDA_bloco_3',conn,if_exists='replace',index=False)\n",
    "df_blc4_appd.to_sql('CDA_bloco_4',conn,if_exists='replace',index=False)\n",
    "df_blc5_appd.to_sql('CDA_bloco_5',conn,if_exists='replace',index=False)\n",
    "df_blc6_appd.to_sql('CDA_bloco_6',conn,if_exists='replace',index=False)\n",
    "df_blc7_appd.to_sql('CDA_bloco_7',conn,if_exists='replace',index=False)\n",
    "df_blc8_appd.to_sql('CDA_bloco_8',conn,if_exists='replace',index=False)\n",
    "df_conf_appd.to_sql('CDA_confidenc',conn,if_exists='replace',index=False)\n",
    "df_fiim_confid_appd .to_sql('CDA_fiim_confid',conn,if_exists='replace',index=False)\n",
    "df_fiim_appd.to_sql('CDA_fiim',conn,if_exists='replace',index=False)\n",
    "df_fi_PL_appd.to_sql('CDA_fi_pl',conn,if_exists='replace',index=False)\n",
    "df_cad_fi.to_sql('CAD_FI_cad_fi_atual',conn,if_exists='replace',index=False)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "#%%\n",
    "#r_df = pd.read_sql(\"select * from bloco_4\",conn)\n",
    "#print(r_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c46e9",
   "metadata": {},
   "source": [
    "#### Dealint with SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe into the SQLite database\n",
    "conn = sqlite3.connect('D:/finance_data/finance_database.db')\n",
    "\n",
    "df.to_sql('B3_companies_sectors',conn,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1faabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all table names from sqlite_master database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "for table in tables: #print names\n",
    "    print(table[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
