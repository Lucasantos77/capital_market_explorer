{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8536624f",
   "metadata": {},
   "source": [
    "# Getting historical quotation from brazilian stock exchange\n",
    "\n",
    "**What ?** Historical quotation data refers to the daily prices of all listed Brazilian securities traded on the B3 stock exchange. For each asset, such as stocks, funds, BDRs, options, futures, and many other financial instruments, the open, high, low, and closing prices are provided. In addition to these prices, the number of trades and the total daily volume traded are also available.\n",
    "\n",
    "**Why ?** Quotes data is crucial in the context of quantitative trading and asset allocation. It enables algorithmic strategies, market timing, backtesting, and risk management, all of which are essential for effective asset allocation. \n",
    "\n",
    "**How ?** The Brazilian stock exchange (B3) publishes a file on its website containing historical daily asset prices in a fixed-width .csv format. The last five years of price data (2019 - 2024) were manually downloaded from the website. This notebook demonstrates how to read the .csv files within each downloaded .zip file and apply data cleaning and transformation according to this guide[(link here)](https://www.b3.com.br/data/files/33/67/B9/50/D84057102C784E47AC094EA8/SeriesHistoricas_Layout.pdf). At the end, the cleaned data is uploaded to a local SQLite database for further analysis. ***It is important to note that, at this stage, the historical prices are not adjusted for dividend distributions and split/inplit events.***\n",
    "\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1e-hu9egDMB2j2ZoRQLKXd0qd0GTLuXmL\" alt=\"texto_alternativo\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c243b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5a5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import sqlite3\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84223f5a",
   "metadata": {},
   "source": [
    "## Defining paths and constant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9461eb8",
   "metadata": {},
   "source": [
    "####  Looking for new files manually downloaded from B3 website into a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22d4ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2019.ZIP',\n",
       " 'C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2020.ZIP',\n",
       " 'C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2021.ZIP',\n",
       " 'C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2022.ZIP',\n",
       " 'C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2023.ZIP',\n",
       " 'C:\\\\Users\\\\lucas\\\\OneDrive\\\\CM_Explorer\\\\data_scraping\\\\B3_historical_quotes\\\\temp_files\\\\COTAHIST_A2024.ZIP']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_directory = os.getcwd() #getting the script directory path\n",
    "file_path = os.path.join(script_directory,'temp_files') # Define the file path within the subfolder\n",
    "all_files = os.listdir(file_path)\n",
    "zip_files_with_paths = [os.path.join(file_path, file) for file in all_files if file.endswith(('.ZIP','.zip'))]\n",
    "\n",
    "zip_files_with_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0486d1b",
   "metadata": {},
   "source": [
    "#### Defining constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab09b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colspecs = [(0,2),(2,10),(10,12),(12,24),(24,27),(27,39),(39,49), \\\n",
    "            (49,52),(52,56),(56,69),(69,82),(82,95),(95,108),(108,121),\\\n",
    "            (121,134),(134,147),(147,152),(152,170),(170,188),(188,201),\\\n",
    "            (201,202),(202,210),(210,217),(217,230),(230,242),(242,None)] # defining the fixed width of the columns based on the reference document: https://www.b3.com.br/data/files/33/67/B9/50/D84057102C784E47AC094EA8/SeriesHistoricas_Layout.pdf \n",
    "\n",
    "names= ['TIPREG','DT_PREGAO','CODBDI','CODNEG','TPMERC','NOMRES',\\\n",
    "        'ESPECI','PRAZOT','MODREF','PREABE','PREMAX','PREMIN','PREMED',\\\n",
    "        'PREULT','PREOFC','PREOFV','TOTNEG','QUATOT','VOLTOT','PREEXE',\\\n",
    "        'INDOPC','DATVEN','FATCOT','PTOEXE','CODISI','DISMES'] # defing columns names\n",
    "\n",
    "# reding support table with descriptions for some columns values\n",
    "df_codbdi = pd.read_excel(script_directory+'\\\\support_table.xlsx', sheet_name='CODBDI')\n",
    "df_especi = pd.read_excel(script_directory+'\\\\support_table.xlsx', sheet_name='ESPECI')\n",
    "df_tpmerc = pd.read_excel(script_directory+'\\\\support_table.xlsx', sheet_name='TPMERC')\n",
    "df_indopc = pd.read_excel(script_directory+'\\\\support_table.xlsx', sheet_name='INDOPC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28bb19",
   "metadata": {},
   "source": [
    "##  Defining functions to prep and save dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d24a51",
   "metadata": {},
   "source": [
    "#### Clean and prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e3b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_data_cleaning(df,df_codbdi,df_especi,df_tpmerc,df_indopc):\n",
    "\n",
    "    # adjusting prices \n",
    "    df['PREABE'] = df['PREABE']/100\n",
    "    df['PREMAX'] = df['PREMAX']/100\n",
    "    df['PREMIN'] = df['PREMIN']/100\n",
    "    df['PREMED'] = df['PREMED']/100\n",
    "    df['PREULT'] = df['PREULT']/100\n",
    "    df['PREOFC'] = df['PREOFC']/100\n",
    "    df['PREOFV'] = df['PREOFV']/100\n",
    "    df['VOLTOT'] = df['VOLTOT']/100\n",
    "    df['PREEXE'] = df['PREEXE']/100\n",
    "    # bring description values for some columns from support table \n",
    "    df = pd.merge(df,df_codbdi, how = 'left', on = ['CODBDI','CODBDI'])\n",
    "    df = pd.merge(df,df_especi, how = 'left', on = ['ESPECI','ESPECI'])\n",
    "    df = pd.merge(df,df_tpmerc, how = 'left', on = ['TPMERC','TPMERC'])\n",
    "    df = pd.merge(df,df_indopc, how = 'left', on = ['INDOPC','INDOPC'])\n",
    "    # changing data types\n",
    "    df['DT_PREGAO'] = df['DT_PREGAO'].astype(str).str[:4] + '-' + df['DT_PREGAO'].astype(str).str[4:6] + '-' + df['DT_PREGAO'].astype(str).str[6:]\n",
    "    df['DATVEN'] = df['DATVEN'].astype(str).str[:4] + '-' + df['DATVEN'].astype(str).str[4:6] + '-' + df['DATVEN'].astype(str).str[6:]\n",
    "    df['pregao_dt'] = pd.to_datetime(df['DT_PREGAO']) # adding also a date_time type \n",
    "    # adding a processing data column to referece the date of the scraping process run\n",
    "    df['proc_datedt'] = datetime.now().replace(microsecond=0) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02478e",
   "metadata": {},
   "source": [
    "#### Save dataframe into a local SQLite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5041272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_write_sqldatabase(df):\n",
    "    conn = sqlite3.connect(os.getenv('MY_FINANCE_DB_PATH')+'/finance_database.db')\n",
    "    \n",
    "    df.to_sql('B3_historical_quotes',conn,if_exists='append',index=False)\n",
    "    \n",
    "    conn.close()\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057048c",
   "metadata": {},
   "source": [
    "## Loop to read the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba9554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File read: COTAHIST_A2019.TXT \n",
      "\n",
      "\n",
      " File read: COTAHIST_A2020.TXT \n",
      "\n",
      "\n",
      " File read: COTAHIST_A2021.TXT \n",
      "\n",
      "\n",
      " File read: COTAHIST_A2022.TXT \n",
      "\n",
      "\n",
      " File read: COTAHIST_A2023.TXT \n",
      "\n",
      "\n",
      " File read: COTAHIST_A2024.TXT \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for zip_file_name in zip_files_with_paths:\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_file:\n",
    "        \n",
    "        csv_file_name = zip_file.namelist()[0]\n",
    "        \n",
    "        df = pd.read_fwf(zip_file.open(csv_file_name),skiprows=2,skipfooter=1,\\\n",
    "                  colspecs=colspecs,names=names,header=None,index_col=False,encoding='mbcs')\n",
    "        \n",
    "        # applying cleaning and trasnformations over the dataframe\n",
    "        df = func_data_cleaning(df,df_codbdi,df_especi,df_tpmerc,df_indopc)\n",
    "        \n",
    "        # writting the final dataframe into a local SQL lite database\n",
    "        func_write_sqldatabase(df)\n",
    "        # deleting dataframe for the next year file reading\n",
    "        del df\n",
    "        \n",
    "        print('\\n File read: {}'.format(csv_file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
